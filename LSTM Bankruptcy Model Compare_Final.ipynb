{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f794c6af-b74a-43d5-85fd-d48e678aa21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix, classification_report, roc_auc_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e64eee4-193b-4d80-8567-b6805f294e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"sampled_10_percent.csv\", delimiter=',')\n",
    "df['public_date'] = pd.to_datetime(df['public_date'])\n",
    "df = df.sort_values(by=['permno', 'public_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcef12eb-f9ce-4465-9e97-6cb15f952eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the last entry for each company\n",
    "last_entry_df = df.groupby('permno').last().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a46513-6756-47d9-be73-052f1ee54ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "features = ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10',\n",
    "          'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19',\n",
    "          'X20', 'X21', 'X22', 'X23', 'X24', 'X25', 'X26', 'X27', 'X28',\n",
    "          'X29', 'X30', 'X31', 'X32', 'X33', 'X34', 'X35', 'X36', 'X37',\n",
    "          'X38', 'X39', 'X40', 'X41', 'X42', 'X43', 'X44', 'X45', 'X46',\n",
    "          'X47', 'X48', 'X49', 'X50', 'X51', 'X52', 'X53', 'X54', 'X55',\n",
    "          'X56', 'X57', 'X58', 'X59', 'X60', 'X61', 'X62', 'X63', 'X64',\n",
    "          'X65', 'X66', 'X67', 'X68', 'X69', 'X70', 'X71']\n",
    "\n",
    "target = 'Bankruptcy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861d694e-ab34-442f-8a49-9ff2be55bbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create masks for train, test, and out-sample datasets based on the last entry dates\n",
    "train_mask = (last_entry_df['public_date'] >= '1970-01-01') & (last_entry_df['public_date'] <= '2010-12-31')\n",
    "test_mask = (last_entry_df['public_date'] >= '1970-01-01') & (last_entry_df['public_date'] <= '2020-12-31')\n",
    "out_sample_mask = (last_entry_df['public_date'] >= '2011-01-01') & (last_entry_df['public_date'] <= '2020-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b2ae83-4b5c-46af-a10d-beb3c305ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the datasets\n",
    "train_df = last_entry_df[train_mask]\n",
    "test_df = last_entry_df[test_mask]\n",
    "out_sample_df = last_entry_df[out_sample_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08dc67b-1b80-4ff0-9fb8-7135a551f670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and target\n",
    "X_train = train_df[features].values\n",
    "y_train = train_df[target].values\n",
    "X_test = test_df[features].values\n",
    "y_test = test_df[target].values\n",
    "X_out = out_sample_df[features].values\n",
    "y_out = out_sample_df[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920a90cf-40b0-4c16-9c95-3b881f3d8ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "X_out = imputer.transform(X_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03867af-fd31-496b-8fa0-a501f6dd6ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_out_scaled = scaler.transform(X_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409e4255-217c-44c3-b2bf-9d7cfa174923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the dataset using SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = sm.fit_resample(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0162e3-438e-485c-a7a8-424b249c7403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build MDA model\n",
    "lda = LDA(n_components=None, solver='svd', tol=0.0001)\n",
    "lda.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdff028-20de-40e7-9bb5-c78ab8bce4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions Training Data\n",
    "y_pred = lda.predict(X_train_scaled)\n",
    "y_pred_probs = lda.predict_proba(X_train_scaled)[:, 1]  # assuming the positive class is at index 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b4a58f-34ab-48e4-a285-bbcebcc23140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "train_accuracy = accuracy_score(y_train, y_pred)\n",
    "train_precision = precision_score(y_train, y_pred)\n",
    "train_recall = recall_score(y_train, y_pred)\n",
    "train_f1 = f1_score(y_train, y_pred)\n",
    "train_roc_auc = roc_auc_score(y_train, y_pred_probs)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Train Precision: {train_precision:.4f}\")\n",
    "print(f\"Train Recall: {train_recall:.4f}\")\n",
    "print(f\"Train F1 Score: {train_f1:.4f}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5a43db-a3ad-42cd-bc02-a5ecd39ba6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the confusion matrix Training Data \n",
    "cm = confusion_matrix(y_train, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2b26d6-d615-4fad-ab3b-dfa3a64a3de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions Testing Data\n",
    "y_pred = lda.predict(X_test_scaled)\n",
    "y_pred_probs = lda.predict_proba(X_test_scaled)[:, 1]  # assuming the positive class is at index 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b3cd1f-0b0c-4ffd-8eed-79427431c902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "test_precision = precision_score(y_test, y_pred)\n",
    "test_recall = recall_score(y_test, y_pred)\n",
    "test_f1 = f1_score(y_test, y_pred)\n",
    "test_roc_auc = roc_auc_score(y_test, y_pred_probs)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "print(f\"Test ROC AUC: {test_roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1af5e70-eda9-46d7-905b-483805e052c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4657a4dc-f788-4dbb-89ef-5ac6cdb17f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions Out-of-Sample LDA\n",
    "y_pred = lda.predict(X_out_scaled)\n",
    "y_pred_probs = lda.predict_proba(X_out_scaled)[:, 1]  # assuming the positive class is at index 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631d99ba-7c2a-46c2-af74-82ee89413e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "out_accuracy = accuracy_score(y_out, y_pred)\n",
    "out_precision = precision_score(y_out, y_pred)\n",
    "out_recall = recall_score(y_out, y_pred)\n",
    "out_f1 = f1_score(y_out, y_pred)\n",
    "out_roc_auc = roc_auc_score(y_out, y_pred_probs)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Out Accuracy: {out_accuracy:.4f}\")\n",
    "print(f\"Out Precision: {out_precision:.4f}\")\n",
    "print(f\"Out Recall: {out_recall:.4f}\")\n",
    "print(f\"Out F1 Score: {out_f1:.4f}\")\n",
    "print(f\"Out ROC AUC: {out_roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b737fc-5d95-4fae-a9fe-7724e38ac303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the confusion matrix\n",
    "cm = confusion_matrix(y_out, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0c2c95-f5f3-440d-8d4f-22bf0e29815f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=10000, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "log_reg.fit(X_train_resampled, y_train_resampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e235b32-2281-4b4e-b405-e5d1b1c07d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the train data\n",
    "y_pred_lr = log_reg.predict(X_train_scaled)\n",
    "y_pred_probs_lr = log_reg.predict_proba(X_train_scaled)[:, 1]  # probabilities for the positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61680e0-9d8d-4e98-a691-4bef2003495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "train_accuracy_lr = accuracy_score(y_train, y_pred_lr)\n",
    "train_precision_lr = precision_score(y_train, y_pred_lr, zero_division=1)\n",
    "train_recall_lr = recall_score(y_train, y_pred_lr, zero_division=1)\n",
    "train_f1_lr = f1_score(y_train, y_pred_lr, zero_division=1)\n",
    "train_roc_auc_lr = roc_auc_score(y_train, y_pred_probs_lr)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Logistic Regression Train Accuracy: {train_accuracy_lr:.4f}\")\n",
    "print(f\"Logistic Regression Train Precision: {train_precision_lr:.4f}\")\n",
    "print(f\"Logistic Regression Train Recall: {train_recall_lr:.4f}\")\n",
    "print(f\"Logistic Regression Train F1 Score: {train_f1_lr:.4f}\")\n",
    "print(f\"Logistic Regression Train ROC AUC: {train_roc_auc_lr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c15344-b79b-4404-9b20-f1c282a49ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the confusion matrix\n",
    "cm = confusion_matrix(y_train, y_pred_lr)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdd2dcd-20ee-423d-a380-8d2c937f4c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred_lr = log_reg.predict(X_test_scaled)\n",
    "y_pred_probs_lr = log_reg.predict_proba(X_test_scaled)[:, 1]  # probabilities for the positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15da8d7d-e377-43bd-a6f5-4320deafa382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "test_precision_lr = precision_score(y_test, y_pred_lr, zero_division=1)\n",
    "test_recall_lr = recall_score(y_test, y_pred_lr, zero_division=1)\n",
    "test_f1_lr = f1_score(y_test, y_pred_lr, zero_division=1)\n",
    "test_roc_auc_lr = roc_auc_score(y_test, y_pred_probs_lr)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Logistic Regression Test Accuracy: {test_accuracy_lr:.4f}\")\n",
    "print(f\"Logistic Regression Test Precision: {test_precision_lr:.4f}\")\n",
    "print(f\"Logistic Regression Test Recall: {test_recall_lr:.4f}\")\n",
    "print(f\"Logistic Regression Test F1 Score: {test_f1_lr:.4f}\")\n",
    "print(f\"Logistic Regression Test ROC AUC: {test_roc_auc_lr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125aa60e-560a-4d53-8f98-65b6437c4d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_lr)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dd92af-0f19-44d5-ad18-9f317a39b747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the out-of-sample data\n",
    "y_pred_lr = log_reg.predict(X_out_scaled)\n",
    "y_pred_probs_lr = log_reg.predict_proba(X_out_scaled)[:, 1]  # probabilities for the positive class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8a3bdd-e085-46ce-bfbd-68243f321b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "out_accuracy_lr = accuracy_score(y_out, y_pred_lr)\n",
    "out_precision_lr = precision_score(y_out, y_pred_lr, zero_division=1)\n",
    "out_recall_lr = recall_score(y_out, y_pred_lr, zero_division=1)\n",
    "out_f1_lr = f1_score(y_out, y_pred_lr, zero_division=1)\n",
    "out_roc_auc_lr = roc_auc_score(y_out, y_pred_probs_lr)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Logistic Regression out Accuracy: {out_accuracy_lr:.4f}\")\n",
    "print(f\"Logistic Regression out Precision: {out_precision_lr:.4f}\")\n",
    "print(f\"Logistic Regression out Recall: {out_recall_lr:.4f}\")\n",
    "print(f\"Logistic Regression out F1 Score: {out_f1_lr:.4f}\")\n",
    "print(f\"Logistic Regression out ROC AUC: {out_roc_auc_lr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc15350a-5ded-4c98-9dba-bb2ac8ddd848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the confusion matrix\n",
    "cm = confusion_matrix(y_out, y_pred_lr)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16085642-da54-4ad7-8926-7216254a2388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the Random Forest model with adjusted parameters\n",
    "random_forest = RandomForestClassifier(\n",
    "    n_estimators=50,  # Reduced number of trees\n",
    "    max_depth=10,     # Limiting depth of each tree\n",
    "    n_jobs=-1,        # Use all available cores\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "random_forest.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a8cec1-eb78-479a-8386-66f8434a600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the train data\n",
    "y_pred_rf = random_forest.predict(X_train_scaled)\n",
    "y_pred_probs_rf = random_forest.predict_proba(X_train_scaled)[:, 1]  # probabilities for the positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f6526c-102e-40ac-8dc5-68c347d91f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "train_accuracy_rf = accuracy_score(y_train, y_pred_rf)\n",
    "train_precision_rf = precision_score(y_train, y_pred_rf, zero_division=1)\n",
    "train_recall_rf = recall_score(y_train, y_pred_rf, zero_division=1)\n",
    "train_f1_rf = f1_score(y_train, y_pred_rf, zero_division=1)\n",
    "train_roc_auc_rf = roc_auc_score(y_train, y_pred_probs_rf)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Random Forest Train Accuracy: {train_accuracy_rf:.4f}\")\n",
    "print(f\"Random Forest Train Precision: {train_precision_rf:.4f}\")\n",
    "print(f\"Random Forest Train Recall: {train_recall_rf:.4f}\")\n",
    "print(f\"Random Forest Train F1 Score: {train_f1_rf:.4f}\")\n",
    "print(f\"Random Forest Train ROC AUC: {train_roc_auc_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994fabd1-6667-44fb-9469-d5890db3b85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the confusion matrix\n",
    "cm = confusion_matrix(y_train, y_pred_rf)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbc95c4-a5fd-4bc0-88df-4a1b812b3bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred_rf = random_forest.predict(X_test_scaled)\n",
    "y_pred_probs_rf = random_forest.predict_proba(X_test_scaled)[:, 1]  # probabilities for the positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ac6ffb-e704-4837-8422-a339d8a85be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "test_precision_rf = precision_score(y_test, y_pred_rf, zero_division=1)\n",
    "test_recall_rf = recall_score(y_test, y_pred_rf, zero_division=1)\n",
    "test_f1_rf = f1_score(y_test, y_pred_rf, zero_division=1)\n",
    "test_roc_auc_rf = roc_auc_score(y_test, y_pred_probs_rf)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Random Forest Test Accuracy: {test_accuracy_rf:.4f}\")\n",
    "print(f\"Random Forest Test Precision: {test_precision_rf:.4f}\")\n",
    "print(f\"Random Forest Test Recall: {test_recall_rf:.4f}\")\n",
    "print(f\"Random Forest Test F1 Score: {test_f1_rf:.4f}\")\n",
    "print(f\"Random Forest Test ROC AUC: {test_roc_auc_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b066ee-ac1b-47d5-a6fc-c2fa9963f537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_rf)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265792ad-641d-4a8f-87b4-b0be16ac7dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the out-of-sample data\n",
    "y_pred_rf = random_forest.predict(X_out_scaled)\n",
    "y_pred_probs_rf = random_forest.predict_proba(X_out_scaled)[:, 1]  # probabilities for the positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c79d4aa-eba2-4cee-93a6-1554180460d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_accuracy_rf = accuracy_score(y_out, y_pred_rf)\n",
    "out_precision_rf = precision_score(y_out, y_pred_rf, zero_division=1)\n",
    "out_recall_rf = recall_score(y_out, y_pred_rf, zero_division=1)\n",
    "out_f1_rf= f1_score(y_out, y_pred_rf, zero_division=1)\n",
    "out_roc_auc_rf = roc_auc_score(y_out, y_pred_probs_rf)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Random Forest out Accuracy: {out_accuracy_rf:.4f}\")\n",
    "print(f\"Random Forest out Precision: {out_precision_rf:.4f}\")\n",
    "print(f\"Random Forest out Recall: {out_recall_rf:.4f}\")\n",
    "print(f\"Random Forest out F1 Score: {out_f1_rf:.4f}\")\n",
    "print(f\"Random Forest out ROC AUC: {out_roc_auc_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a395cd6b-53e2-46cb-acc9-d2464a0832e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the confusion matrix\n",
    "cm = confusion_matrix(y_out, y_pred_rf)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a2d90e-b50e-48a8-b895-84bdaf69b38c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
