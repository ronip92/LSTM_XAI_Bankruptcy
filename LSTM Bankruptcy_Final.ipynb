{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f794c6af-b74a-43d5-85fd-d48e678aa21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, cohen_kappa_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16308d35-ac80-4d5e-8aad-38a33f07b185",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e64eee4-193b-4d80-8567-b6805f294e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df1 = pd.read_csv(\"sampled_10_percent.csv\", delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc1dfaa-70a1-4d03-8aa0-923f89064af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a46513-6756-47d9-be73-052f1ee54ff8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert 'public_date' to datetime and sort data\n",
    "df1['public_date'] = pd.to_datetime(df1['public_date'])\n",
    "df1 = df1.sort_values(by=['permno', 'public_date'])\n",
    "\n",
    "# Get unique company counts before filtering\n",
    "initial_bankrupt_count = df1[df1['Bankrupt']].drop_duplicates('permno').shape[0]\n",
    "initial_non_bankrupt_count = df1[~df1['Bankrupt']].drop_duplicates('permno').shape[0]\n",
    "\n",
    "# Specify the periods, adjusted for zero-based index\n",
    "periods = [x - 1 for x in [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36]]\n",
    "\n",
    "# Filter rows for each 'permno' based on the specified periods\n",
    "filtered_df1 = df1.groupby('permno').nth(periods).reset_index()\n",
    "\n",
    "# Count unique bankrupt and non-bankrupt companies post-filtering\n",
    "filtered_bankrupt_count = filtered_df1[filtered_df1['Bankrupt']].drop_duplicates('permno').shape[0]\n",
    "filtered_non_bankrupt_count = filtered_df1[~filtered_df1['Bankrupt']].drop_duplicates('permno').shape[0]\n",
    "\n",
    "# Print the initial and post-filtering counts\n",
    "print(f\"Initial Number of bankrupt companies: {initial_bankrupt_count}\")\n",
    "print(f\"Initial Number of non-bankrupt companies: {initial_non_bankrupt_count}\")\n",
    "print(f\"Filtered Number of bankrupt companies: {filtered_bankrupt_count}\")\n",
    "print(f\"Filtered Number of non-bankrupt companies: {filtered_non_bankrupt_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f94554-8c07-4a91-89c1-76808a233625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratios\n",
    "ratios = ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10',\n",
    "          'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19',\n",
    "          'X20', 'X21', 'X22', 'X23', 'X24', 'X25', 'X26', 'X27', 'X28',\n",
    "          'X29', 'X30', 'X31', 'X32', 'X33', 'X34', 'X35', 'X36', 'X37',\n",
    "          'X38', 'X39', 'X40', 'X41', 'X42', 'X43', 'X44', 'X45', 'X46',\n",
    "          'X47', 'X48', 'X49', 'X50', 'X51', 'X52', 'X53', 'X54', 'X55',\n",
    "          'X56', 'X57', 'X58', 'X59', 'X60', 'X61', 'X62', 'X63', 'X64',\n",
    "          'X65', 'X66', 'X67', 'X68', 'X69', 'X70', 'X71']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75378dc4-4d1e-43d9-b759-bcb42b4a4b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset for LSTM\n",
    "X, y = [], []\n",
    "grouped = filtered_df1.groupby('permno')\n",
    "sequence_length = 12\n",
    "\n",
    "for _, group in grouped:\n",
    "    group = group.sort_values(by='public_date')\n",
    "    if len(group) >= sequence_length:\n",
    "        X.append(group[ratios].tail(sequence_length).values)\n",
    "        y.append(group['Bankruptcy'].iloc[-1])\n",
    "\n",
    "X, y = np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b2ae83-4b5c-46af-a10d-beb3c305ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset based on the date ranges\n",
    "train_mask = (filtered_df1['public_date'] >= '1970-01-01') & (filtered_df1['public_date'] <= '2010-12-31')\n",
    "test_mask = (filtered_df1['public_date'] >= '1970-01-01') & (filtered_df1['public_date'] <= '2020-12-31')\n",
    "out_sample_mask = (filtered_df1['public_date'] >= '2011-01-01') & (filtered_df1['public_date'] <= '2020-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce17b25-dda9-4a59-baa5-71d9c9daa5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([X[i] for i in range(len(X)) if train_mask.iloc[i]])\n",
    "y_train = np.array([y[i] for i in range(len(y)) if train_mask.iloc[i]])\n",
    "X_test = np.array([X[i] for i in range(len(X)) if test_mask.iloc[i]])\n",
    "y_test = np.array([y[i] for i in range(len(y)) if test_mask.iloc[i]])\n",
    "X_out = np.array([X[i] for i in range(len(X)) if out_sample_mask.iloc[i]])\n",
    "y_out = np.array([y[i] for i in range(len(y)) if out_sample_mask.iloc[i]])\n",
    "\n",
    "# Count unique bankrupt and non-bankrupt companies in the training set\n",
    "train_bankrupt_count = filtered_df1[train_mask & filtered_df1['Bankrupt']].drop_duplicates('permno').shape[0]\n",
    "train_non_bankrupt_count = filtered_df1[train_mask & ~filtered_df1['Bankrupt']].drop_duplicates('permno').shape[0]\n",
    "\n",
    "# Count unique bankrupt and non-bankrupt companies in the testing set\n",
    "test_bankrupt_count = filtered_df1[test_mask & filtered_df1['Bankrupt']].drop_duplicates('permno').shape[0]\n",
    "test_non_bankrupt_count = filtered_df1[test_mask & ~filtered_df1['Bankrupt']].drop_duplicates('permno').shape[0]\n",
    "\n",
    "# Print the counts\n",
    "print(f\"Training data shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}, {y_test.shape}\")\n",
    "print(f\"Training bankrupt companies: {train_bankrupt_count}\")\n",
    "print(f\"Training non-bankrupt companies: {train_non_bankrupt_count}\")\n",
    "print(f\"Testing bankrupt companies: {test_bankrupt_count}\")\n",
    "print(f\"Testing non-bankrupt companies: {test_non_bankrupt_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920a90cf-40b0-4c16-9c95-3b881f3d8ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train = imputer.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_test = imputer.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "X_out = imputer.transform(X_out.reshape(-1, X_out.shape[-1])).reshape(X_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03867af-fd31-496b-8fa0-a501f6dd6ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "X_out_scaled = scaler.transform(X_out.reshape(-1, X_out.shape[-1])).reshape(X_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409e4255-217c-44c3-b2bf-9d7cfa174923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the dataset using SMOTE\n",
    "X_train_flattened = X_train_scaled.reshape(X_train_scaled.shape[0], -1)\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = sm.fit_resample(X_train_flattened, y_train)\n",
    "X_train_resampled = X_train_resampled.reshape(-1, sequence_length, X_train_scaled.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de070ee-8f15-4329-a873-058b5f6ecb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KFold and EarlyStopping setup\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714d0c80-0075-4819-95c7-0914dae289a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store metrics\n",
    "avg_accuracy = []\n",
    "avg_recall = []\n",
    "avg_precision = []\n",
    "avg_f1 = []\n",
    "avg_roc_auc = []\n",
    "avg_kappa = []\n",
    "avg_type_ii_error = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26da45e4-3dde-4ff2-9b2c-55f576c68fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start KFold training\n",
    "fold_var = 1\n",
    "for train_index, val_index in kf.split(X_train_resampled):\n",
    "    X_train_fold, X_val_fold = X_train_resampled[train_index], X_train_resampled[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_resampled[train_index], y_train_resampled[val_index]\n",
    "    \n",
    "    # Build the LSTM model\n",
    "    model = Sequential([\n",
    "        LSTM(50, input_shape=(sequence_length, len(ratios)), activation='tanh'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train_fold, y_train_fold, validation_data=(X_val_fold, y_val_fold),\n",
    "                        epochs=20, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "    # Predictions for this fold\n",
    "    y_pred = model.predict(X_val_fold)\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "    # Calculate metrics for this fold\n",
    "    accuracy = accuracy_score(y_val_fold, y_pred_binary)\n",
    "    recall = recall_score(y_val_fold, y_pred_binary)\n",
    "    precision = precision_score(y_val_fold, y_pred_binary)\n",
    "    f1 = f1_score(y_val_fold, y_pred_binary)\n",
    "    roc_auc = roc_auc_score(y_val_fold, y_pred)\n",
    "    kappa = cohen_kappa_score(y_val_fold, y_pred_binary)\n",
    "    type_ii_error = 1 - recall\n",
    "\n",
    "    print(f'Fold {fold_var}:')\n",
    "    print(f'  Accuracy: {accuracy:.4f}')\n",
    "    print(f'  Recall: {recall:.4f}')\n",
    "    print(f'  Precision: {precision:.4f}')\n",
    "    print(f'  F1 Score: {f1:.4f}')\n",
    "    print(f'  ROC AUC: {roc_auc:.4f}')\n",
    "    print(f'  Kappa: {kappa:.4f}')\n",
    "    print(f'  Type II Error: {type_ii_error:.4f}')\n",
    "    print()\n",
    "\n",
    "    # Append metrics for this fold to the lists\n",
    "    avg_accuracy.append(accuracy)\n",
    "    avg_recall.append(recall)\n",
    "    avg_precision.append(precision)\n",
    "    avg_f1.append(f1)\n",
    "    avg_roc_auc.append(roc_auc)\n",
    "    avg_kappa.append(kappa)\n",
    "    avg_type_ii_error.append(type_ii_error)\n",
    "\n",
    "    fold_var += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f9f819-d90f-402c-91a3-f31ca363bb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average of all folds\n",
    "print('Average metrics across all folds:')\n",
    "print(f'  Average Accuracy: {np.mean(avg_accuracy):.4f}')\n",
    "print(f'  Average Recall: {np.mean(avg_recall):.4f}')\n",
    "print(f'  Average Precision: {np.mean(avg_precision):.4f}')\n",
    "print(f'  Average F1 Score: {np.mean(avg_f1):.4f}')\n",
    "print(f'  Average ROC AUC: {np.mean(avg_roc_auc):.4f}')\n",
    "print(f'  Average Kappa: {np.mean(avg_kappa):.4f}')\n",
    "print(f'  Average Type II Error: {np.mean(avg_type_ii_error):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d27e83-e81e-4b4c-a979-cc7dc3028316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the train set\n",
    "y_train_pred_probs = model.predict(X_train_scaled)\n",
    "y_train_pred = (y_train_pred_probs > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4de574-0071-47f1-bfd8-50f9075936cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_precision = precision_score(y_train, y_train_pred)\n",
    "train_recall = recall_score(y_train, y_train_pred)\n",
    "train_f1 = f1_score(y_train, y_train_pred)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_pred_probs)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Train Precision: {train_precision:.4f}\")\n",
    "print(f\"Train Recall: {train_recall:.4f}\")\n",
    "print(f\"Train F1 Score: {train_f1:.4f}\")\n",
    "print(f\"Train ROC AUC: {train_roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1af5e70-eda9-46d7-905b-483805e052c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the confusion matrix\n",
    "cm = confusion_matrix(y_train, y_train_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aec7832-8cd3-40db-9b9b-8cf3a725f75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_test_pred_probs = model.predict(X_test_scaled)\n",
    "y_test_pred = (y_test_pred_probs > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d46bb48-11d4-4e36-a566-a0837ea6f806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "test_roc_auc = roc_auc_score(y_test, y_test_pred_probs)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "print(f\"Test ROC AUC: {test_roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06676209-ce03-493e-9a52-456e4b2dc717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0c2c95-f5f3-440d-8d4f-22bf0e29815f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the Out of Sample set\n",
    "y_out_pred_probs = model.predict(X_out_scaled)\n",
    "y_out_pred = (y_out_pred_probs > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f939e45-2141-49be-84f0-a122000531df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "Out_accuracy = accuracy_score(y_out, y_out_pred)\n",
    "Out_precision = precision_score(y_out, y_out_pred)\n",
    "Out_recall = recall_score(y_out, y_out_pred)\n",
    "Out_f1 = f1_score(y_out, y_out_pred)\n",
    "Out_roc_auc = roc_auc_score(y_out, y_out_pred_probs)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Out of Sample Accuracy: {Out_accuracy:.4f}\")\n",
    "print(f\"Out of Sample Precision: {Out_precision:.4f}\")\n",
    "print(f\"Out of Sample Recall: {Out_recall:.4f}\")\n",
    "print(f\"Out of Sample F1 Score: {Out_f1:.4f}\")\n",
    "print(f\"Out of Sample ROC AUC: {Out_roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2f93c0-5ab7-469f-b1bd-84b3fb75fd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the confusion matrix\n",
    "cm = confusion_matrix(y_out, y_out_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46fd8e8-69a2-42c3-855c-8ec1ac707ff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
